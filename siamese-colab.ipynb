{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LRPnPcDIByl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !google-drive-ocamlfuse drive\n",
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14aeb-3A9TYO",
        "colab_type": "text"
      },
      "source": [
        "### Pip install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBPiL4Aoqjwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install pillow\n",
        "!pip install tqdm\n",
        "!pip install keras\n",
        "!pip install numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEzDkyQOs-AB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O10BFXSipobN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DataLoader(object):\n",
        "    \"\"\"\n",
        "    Class for loading data from image files\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, width, height, cells, data_path, output_path):\n",
        "        \"\"\"\n",
        "        Proper width and height for each image.\n",
        "        \"\"\"\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.cells = cells\n",
        "        self.data_path = data_path\n",
        "        self.output_path = output_path\n",
        "\n",
        "    def _open_image(self, path):\n",
        "        \"\"\"\n",
        "        Using the Image library we open the image in the given path. The path must lead to a .jpg file.\n",
        "        We then resize it to 105x105 like in the paper (the dataset contains 250x250 images.)\n",
        "\n",
        "        Returns the image as a numpy array.\n",
        "        \"\"\"\n",
        "        image = Image.open(path)\n",
        "        image = image.resize((self.width, self.height))\n",
        "        data = np.asarray(image)\n",
        "        data = np.array(data, dtype='float64')\n",
        "        return data\n",
        "\n",
        "    def convert_image_to_array(self, person, image_num, data_path, predict=False):\n",
        "        \"\"\"\n",
        "        Given a person, image number and datapath, returns a numpy array which represents the image.\n",
        "        predict - whether this function is called during training or testing. If called when training, we must reshape\n",
        "        the images since the given dataset is not in the correct dimensions.\n",
        "        \"\"\"\n",
        "        max_zeros = 4\n",
        "        image_num = '0' * max_zeros + image_num\n",
        "        image_num = image_num[-max_zeros:]\n",
        "        image_path = os.path.join(data_path, 'lfw2', person, f'{person}_{image_num}.jpg')\n",
        "        image_data = self._open_image(image_path)\n",
        "        if not predict:\n",
        "            image_data = image_data.reshape(self.width, self.height, self.cells)\n",
        "        return image_data\n",
        "\n",
        "    def load(self, set_name):\n",
        "        \"\"\"\n",
        "        Writes into the given output_path the images from the data_path.\n",
        "        dataset_type = train or test\n",
        "        \"\"\"\n",
        "        file_path = os.path.join(self.data_path, 'splits', f'{set_name}.txt')\n",
        "        print(file_path)\n",
        "        print('Loading dataset...')\n",
        "        x_first = []\n",
        "        x_second = []\n",
        "        y = []\n",
        "        names = []\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "        for line in tqdm.tqdm(lines):\n",
        "            line = line.split()\n",
        "            if len(line) == 4:  # Class 0 - non-identical\n",
        "                names.append(line)\n",
        "                first_person_name, first_image_num, second_person_name, second_image_num = line[0], line[1], line[2], \\\n",
        "                                                                                           line[3]\n",
        "                first_image = self.convert_image_to_array(person=first_person_name,\n",
        "                                                          image_num=first_image_num,\n",
        "                                                          data_path=self.data_path)\n",
        "                second_image = self.convert_image_to_array(person=second_person_name,\n",
        "                                                           image_num=second_image_num,\n",
        "                                                           data_path=self.data_path)\n",
        "                x_first.append(first_image)\n",
        "                x_second.append(second_image)\n",
        "                y.append(0)\n",
        "            elif len(line) == 3:  # Class 1 - identical\n",
        "                names.append(line)\n",
        "                person_name, first_image_num, second_image_num = line[0], line[1], line[2]\n",
        "                first_image = self.convert_image_to_array(person=person_name,\n",
        "                                                          image_num=first_image_num,\n",
        "                                                          data_path=self.data_path)\n",
        "                second_image = self.convert_image_to_array(person=person_name,\n",
        "                                                           image_num=second_image_num,\n",
        "                                                           data_path=self.data_path)\n",
        "                x_first.append(first_image)\n",
        "                x_second.append(second_image)\n",
        "                y.append(1)\n",
        "            elif len(line) == 1:\n",
        "                print(f'line with a single value: {line}')\n",
        "        print('Done loading dataset')\n",
        "        with open(self.output_path, 'wb') as f:\n",
        "            pickle.dump([[x_first, x_second], y, names], f)\n",
        "\n",
        "\n",
        "print(\"Loaded data loader\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5dO2Y-Bpo_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Input, Sequential, Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Lambda, BatchNormalization, Activation, \\\n",
        "    Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "class SiameseNetwork(object):\n",
        "    def __init__(self, seed, width, height, cells, loss, metrics, optimizer, dropout_rate):\n",
        "        \"\"\"\n",
        "        Seed - The seed used to initialize the weights\n",
        "        width, height, cells - used for defining the tensors used for the input images\n",
        "        loss, metrics, optimizer, dropout_rate - settings used for compiling the siamese model (e.g., 'Accuracy' and 'ADAM)\n",
        "        \"\"\"\n",
        "        K.clear_session()\n",
        "        self.load_file = None\n",
        "        self.seed = seed\n",
        "        self.initialize_seed()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Define the matrices for the input images\n",
        "        input_shape = (width, height, cells)\n",
        "        left_input = Input(input_shape)\n",
        "        right_input = Input(input_shape)\n",
        "\n",
        "        # Get the CNN architecture as presented in the paper (read the readme for more information)\n",
        "        model = self._get_architecture(input_shape)\n",
        "        encoded_l = model(left_input)\n",
        "        encoded_r = model(right_input)\n",
        "\n",
        "        # Add a layer to combine the two CNNs\n",
        "        L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
        "        L1_siamese_dist = L1_layer([encoded_l, encoded_r])\n",
        "        L1_siamese_dist = Dropout(dropout_rate)(L1_siamese_dist)\n",
        "\n",
        "        # An output layer with Sigmoid activation function\n",
        "        prediction = Dense(1, activation='sigmoid', bias_initializer=self.initialize_bias)(L1_siamese_dist)\n",
        "\n",
        "        siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
        "        self.siamese_net = siamese_net\n",
        "        self.siamese_net.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    def initialize_seed(self):\n",
        "        \"\"\"\n",
        "        Initialize seed all for environment\n",
        "        \"\"\"\n",
        "        os.environ['PYTHONHASHSEED'] = str(self.seed)\n",
        "        random.seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "        tf.random.set_seed(self.seed)\n",
        "\n",
        "    def initialize_weights(self, shape, dtype=None):\n",
        "        \"\"\"\n",
        "        Called when initializing the weights of the siamese model, uses the random_normal function of keras to return a\n",
        "        tensor with a normal distribution of weights.\n",
        "        \"\"\"\n",
        "        return K.random_normal(shape, mean=0.0, stddev=0.01, dtype=dtype, seed=self.seed)\n",
        "\n",
        "    def initialize_bias(self, shape, dtype=None):\n",
        "        \"\"\"\n",
        "        Called when initializing the biases of the siamese model, uses the random_normal function of keras to return a\n",
        "        tensor with a normal distribution of weights.\n",
        "        \"\"\"\n",
        "        return K.random_normal(shape, mean=0.5, stddev=0.01, dtype=dtype, seed=self.seed)\n",
        "\n",
        "    def _get_architecture(self, input_shape):\n",
        "        \"\"\"\n",
        "        Returns a Convolutional Neural Network based on the input shape given of the images. This is the CNN network\n",
        "        that is used inside the siamese model. Uses parameters from the siamese one shot paper.\n",
        "        \"\"\"\n",
        "        model = Sequential()\n",
        "        model.add(\n",
        "            Conv2D(filters=64,\n",
        "                   kernel_size=(10, 10),\n",
        "                   input_shape=input_shape,\n",
        "                   kernel_initializer=self.initialize_weights,\n",
        "                   kernel_regularizer=l2(2e-4),\n",
        "                   name='Conv1'\n",
        "                   ))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D())\n",
        "\n",
        "        model.add(\n",
        "            Conv2D(filters=128,\n",
        "                   kernel_size=(7, 7),\n",
        "                   kernel_initializer=self.initialize_weights,\n",
        "                   bias_initializer=self.initialize_bias,\n",
        "                   kernel_regularizer=l2(2e-4),\n",
        "                   name='Conv2'\n",
        "                   ))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D())\n",
        "\n",
        "        model.add(\n",
        "            Conv2D(filters=128,\n",
        "                   kernel_size=(4, 4),\n",
        "                   kernel_initializer=self.initialize_weights,\n",
        "                   bias_initializer=self.initialize_bias,\n",
        "                   kernel_regularizer=l2(2e-4),\n",
        "                   name='Conv3'\n",
        "                   ))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D())\n",
        "\n",
        "        model.add(\n",
        "            Conv2D(filters=256,\n",
        "                   kernel_size=(4, 4),\n",
        "                   kernel_initializer=self.initialize_weights,\n",
        "                   bias_initializer=self.initialize_bias,\n",
        "                   kernel_regularizer=l2(2e-4),\n",
        "                   name='Conv4'\n",
        "                   ))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(\n",
        "            Dense(4096,\n",
        "                  activation='sigmoid',\n",
        "                  kernel_initializer=self.initialize_weights,\n",
        "                  kernel_regularizer=l2(2e-3),\n",
        "                  bias_initializer=self.initialize_bias))\n",
        "        return model\n",
        "\n",
        "    def _load_weights(self, weights_file):\n",
        "        \"\"\"\n",
        "        A function that attempts to load pre-existing weight files for the siamese model. If it succeeds then returns\n",
        "        True and updates the weights, otherwise False.\n",
        "        :return True if the file is already exists\n",
        "        \"\"\"\n",
        "        # self.siamese_net.summary()\n",
        "        self.load_file = weights_file\n",
        "        if os.path.exists(weights_file):  # if the file is already exists, load and return true\n",
        "            print('Loading pre-existed weights file')\n",
        "            self.siamese_net.load_weights(weights_file)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def fit(self, weights_file, train_path, validation_size, batch_size, epochs, early_stopping, patience, min_delta):\n",
        "        \"\"\"\n",
        "        Function for fitting the model. If the weights already exist, just return the summary of the model. Otherwise,\n",
        "        perform a whole train/validation/test split and train the model with the given parameters.\n",
        "        \"\"\"\n",
        "        with open(train_path, 'rb') as f:\n",
        "            x_train, y_train, names = pickle.load(f)\n",
        "        \"\"\"\n",
        "        X_train[0]:  |----------x_train_0---------------------------|-------x_val_0--------|\n",
        "        X_train[1]:  |----------x_train_1---------------------------|-------x_val_1--------|\n",
        "        y_train:     |----------y_train_0 = y_train_1---------------|----y_val_0=y_val_1---|\n",
        "        \"\"\"\n",
        "        x_train_0, x_val_0, y_train_0, y_val_0 = train_test_split(x_train[0], y_train,\n",
        "                                                                  test_size=validation_size,\n",
        "                                                                  random_state=self.seed)\n",
        "        x_train_1, x_val_1, y_train_1, y_val_1 = train_test_split(x_train[1], y_train,\n",
        "                                                                  test_size=validation_size,\n",
        "                                                                  random_state=self.seed)\n",
        "        x_train_0 = np.array(x_train_0, dtype='float64')\n",
        "        x_val_0 = np.array(x_val_0, dtype='float64')\n",
        "        x_train_1 = np.array(x_train_1, dtype='float64')\n",
        "        x_val_1 = np.array(x_val_1, dtype='float64')\n",
        "        x_train = [x_train_0, x_train_1]\n",
        "        x_val = [x_val_0, x_val_1]\n",
        "        if y_train_0 != y_train_1 and y_val_0 != y_val_1:\n",
        "            raise Exception(\"y train lists or y validation list do not equal\")\n",
        "        y_train_both = np.array(y_train_0, dtype='float64')\n",
        "        y_val_both = np.array(y_val_0, dtype='float64')\n",
        "        if not self._load_weights(weights_file=weights_file):\n",
        "            print('No such pre-existed weights file')\n",
        "            print('Beginning to fit the model')\n",
        "            callback = []\n",
        "            if early_stopping:\n",
        "                \"\"\"\n",
        "                We used the EarlyStopping function monitoring on the validation loss with a minimum delta of 0.1\n",
        "                (Minimum change in the monitored quantity to qualify as an improvement, i.e.\n",
        "                an absolute change of less than min_delta, will count as no improvement.) and patience 5 \n",
        "                (Number of epochs with no improvement after which training will be stopped.).\n",
        "                The direction is automatically inferred from the name of the monitored quantity (‘auto’).\n",
        "                \"\"\"\n",
        "                es = EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience, mode='auto', verbose=1)\n",
        "                callback.append(es)\n",
        "            self.siamese_net.fit(x_train, y_train_both, batch_size=batch_size, epochs=epochs,\n",
        "                                 validation_data=(x_val, y_val_both), callbacks=callback, verbose=1)\n",
        "            self.siamese_net.save_weights(self.load_file)\n",
        "        # evaluate on the testing set\n",
        "        loss, accuracy = self.siamese_net.evaluate(x_val, y_val_both, batch_size=batch_size)\n",
        "        print(f'Loss on Validation set: {loss}')\n",
        "        print(f'Accuracy on Validation set: {accuracy}')\n",
        "\n",
        "    def evaluate(self, test_file, batch_size, analyze=False):\n",
        "        \"\"\"\n",
        "        Function for evaluating the final model after training.\n",
        "        test_file - file path to the test file.\n",
        "        batch_size - the batch size used in training.\n",
        "\n",
        "        Returns the loss and accuracy results.\n",
        "        \"\"\"\n",
        "        with open(test_file, 'rb') as f:\n",
        "            x_test, y_test, names = pickle.load(f)\n",
        "        print(f'Available Metrics: {self.siamese_net.metrics_names}')\n",
        "        y_test = np.array(y_test, dtype='float64')\n",
        "        x_test[0] = np.array(x_test[0], dtype='float64')\n",
        "        x_test[1] = np.array(x_test[1], dtype='float64')\n",
        "        # evaluate on the test set\n",
        "        loss, accuracy = self.siamese_net.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "        if analyze:\n",
        "            self._analyze(x_test, y_test, names)\n",
        "        return loss, accuracy\n",
        "\n",
        "    def _analyze(self, x_test, y_test, names):\n",
        "        \"\"\"\n",
        "        Function used for evaluating our network in the methods proposed in the assignment.\n",
        "        We will find:\n",
        "        - The person who has 2 images that are the most dissimilar to each other\n",
        "        - The person with the two images that are the most similar to each other\n",
        "        - Two people with the most dissimilar images, and\n",
        "        - The two people with the most similar images.\n",
        "        \"\"\"\n",
        "        best_class_0_prob = 1  # correct classification for different people, y=0, prediction->0\n",
        "        best_class_0_name = None\n",
        "        worst_class_0_prob = 0  # misclassification for different people, y=0, prediction->1\n",
        "        worst_class_0_name = None\n",
        "        best_class_1_prob = 0  # correct classification for same people, y=1, prediction->1\n",
        "        best_class_1_name = None\n",
        "        worst_class_1_prob = 1  # misclassification for same people, y=1, prediction->0\n",
        "        worst_class_1_name = None\n",
        "        prob = self.siamese_net.predict(x_test)\n",
        "        for pair_index in range(len(names)):\n",
        "            name = names[pair_index]\n",
        "            y_pair = y_test[pair_index]\n",
        "            pair_prob = prob[pair_index][0]\n",
        "            if y_pair == 0:  # different people (actual)\n",
        "                if pair_prob < best_class_0_prob:  # correct classification for different people, y=0, prediction->0\n",
        "                    best_class_0_prob = pair_prob\n",
        "                    best_class_0_name = name\n",
        "                if pair_prob > worst_class_0_prob:  # misclassification for different people, y=0, prediction->1\n",
        "                    worst_class_0_prob = pair_prob\n",
        "                    worst_class_0_name = name\n",
        "            else:  # the same person (actual)\n",
        "                if pair_prob > best_class_1_prob:  # correct classification for same people, y=1, prediction->1\n",
        "                    best_class_1_prob = pair_prob\n",
        "                    best_class_1_name = name\n",
        "                if pair_prob < worst_class_1_prob:  # misclassification for same people, y=1, prediction->0\n",
        "                    worst_class_1_prob = pair_prob\n",
        "                    worst_class_1_name = name\n",
        "\n",
        "        print(f'correct classification for different people, y=0, prediction->0, name: {best_class_0_name} | prob: {best_class_0_prob}')\n",
        "        print(f'misclassification for different people, y=0, prediction->1, name: {worst_class_0_name} | prob: {worst_class_0_prob}')\n",
        "        print(f'correct classification for same people, y=1, prediction->1, name: {best_class_1_name} | prob: {best_class_1_prob}')\n",
        "        print(f'misclassification for same people, y=1, prediction->0, name: {worst_class_1_name} | prob: {worst_class_1_prob}')\n",
        "\n",
        "\n",
        "print(\"Loaded Siamese Network\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohgCVhwgsQSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "path_separator = os.path.sep\n",
        "# Environment settings\n",
        "IS_COLAB = (os.name == 'posix')\n",
        "LOAD_DATA = not (os.name == 'posix')\n",
        "IS_EXPERIMENT = False\n",
        "train_name = 'train'\n",
        "test_name = 'test'\n",
        "WIDTH = HEIGHT = 105\n",
        "CEELS = 1\n",
        "loss_type = \"binary_crossentropy\"\n",
        "validation_size = 0.2\n",
        "early_stopping = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    # the google drive folder we used\n",
        "    data_path = os.path.sep + os.path.join('content', 'drive', 'My\\ Drive', 'datasets', 'lfw2').replace('\\\\', '')\n",
        "else:\n",
        "    # locally\n",
        "    from data_loader import DataLoader\n",
        "    from siamese_network import SiameseNetwork\n",
        "\n",
        "    data_path = os.path.join('lfwa', 'lfw2')\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "\n",
        "def run_combination(l, bs, ep, pat, md, seed, train_path, test_path):\n",
        "    \"\"\"\n",
        "    This function gets the parameters and run the experiment.\n",
        "    :return: loss - loss on the testing set, accuracy - accuracy on the testing set\n",
        "    \"\"\"\n",
        "    # file types\n",
        "    model_save_type = 'h5'\n",
        "    # files paths\n",
        "    initialize_seed(seed)\n",
        "    parameters_name = f'seed_{seed}_lr_{l}_bs_{bs}_ep_{ep}_val_{validation_size}_' \\\n",
        "                      f'es_{early_stopping}_pa_{pat}_md_{md}'\n",
        "    print(f'Running combination with {parameters_name}')\n",
        "    # A path for the weights\n",
        "    load_weights_path = os.path.join(data_path, 'weights', f'weights_{parameters_name}.{model_save_type}')\n",
        "\n",
        "    siamese = SiameseNetwork(seed=seed, width=WIDTH, height=HEIGHT, cells=CEELS, loss=loss_type, metrics=['accuracy'],\n",
        "                             optimizer=Adam(lr=l), dropout_rate=0.4)\n",
        "    siamese.fit(weights_file=load_weights_path, train_path=train_path, validation_size=validation_size,\n",
        "                batch_size=bs, epochs=ep, early_stopping=early_stopping, patience=pat,\n",
        "                min_delta=md)\n",
        "    loss, accuracy = siamese.evaluate(test_file=test_path, batch_size=bs, analyze=True)\n",
        "    print(f'Loss on Testing set: {loss}')\n",
        "    print(f'Accuracy on Testing set: {accuracy}')\n",
        "    # predict_pairs(model)\n",
        "    return loss, accuracy\n",
        "\n",
        "\n",
        "def run():\n",
        "    \"\"\"\n",
        "    The main function that runs the training and experiments. Uses the global variables above.\n",
        "    \"\"\"\n",
        "    # file types\n",
        "    data_set_save_type = 'pickle'\n",
        "    train_path = os.path.join(data_path, f'{train_name}.{data_set_save_type}')  # A path for the train file\n",
        "    test_path = os.path.join(data_path, f'{test_name}.{data_set_save_type}')  # A path for the test file\n",
        "    if LOAD_DATA:  # If the training data already exists\n",
        "        loader = DataLoader(width=WIDTH, height=HEIGHT, cells=CEELS, data_path=data_path, output_path=train_path)\n",
        "        loader.load(set_name=train_name)\n",
        "        loader = DataLoader(width=WIDTH, height=HEIGHT, cells=CEELS, data_path=data_path, output_path=test_path)\n",
        "        loader.load(set_name=test_name)\n",
        "\n",
        "    result_path = os.path.join(data_path, f'results.csv')  # A path for the train file\n",
        "    results = {'lr': [], 'batch_size': [], 'epochs': [], 'patience': [], 'min_delta': [], 'seed': [], 'loss': [],\n",
        "               'accuracy': []}\n",
        "    for l in lr:\n",
        "        for bs in batch_size:\n",
        "            for ep in epochs:\n",
        "                for pat in patience:\n",
        "                    for md in min_delta:\n",
        "                        for seed in seeds:\n",
        "                            loss, accuracy = run_combination(l=l, bs=bs, ep=ep, pat=pat, md=md, seed=seed,\n",
        "                                                             train_path=train_path, test_path=test_path)\n",
        "                            results['lr'].append(l)\n",
        "                            results['batch_size'].append(bs)\n",
        "                            results['epochs'].append(ep)\n",
        "                            results['patience'].append(pat)\n",
        "                            results['min_delta'].append(md)\n",
        "                            results['seed'].append(seed)\n",
        "                            results['loss'].append(loss)\n",
        "                            results['accuracy'].append(accuracy)\n",
        "    df_results = pd.DataFrame.from_dict(results)\n",
        "    df_results.to_csv(result_path)\n",
        "\n",
        "\n",
        "def initialize_seed(seed):\n",
        "    \"\"\"\n",
        "    Initialize all relevant environments with the seed.\n",
        "    \"\"\"\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if IS_EXPERIMENT:\n",
        "        # Experiments settings\n",
        "        seeds = [0]\n",
        "        lr = [0.00005]\n",
        "        batch_size = [32]\n",
        "        epochs = [10]\n",
        "        patience = [5]\n",
        "        min_delta = [0.1]\n",
        "    else:\n",
        "        # Final settings\n",
        "        seeds = [0]\n",
        "        lr = [0.00005]\n",
        "        batch_size = [32]\n",
        "        epochs = [10]\n",
        "        patience = [5]\n",
        "        min_delta = [0.1]\n",
        "\n",
        "    print(os.name)\n",
        "    start_time = time.time()\n",
        "    print('Starting the experiments')\n",
        "    run()\n",
        "    print(f'Total Running Time: {time.time() - start_time}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}